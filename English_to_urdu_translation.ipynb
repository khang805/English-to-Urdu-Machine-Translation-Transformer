{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4873801,"sourceType":"datasetVersion","datasetId":2825963}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================\n# 0) INSTALL PACKAGES\n# ============================================\n!pip install transformers datasets evaluate sacrebleu sentencepiece --quiet\n\n# ============================================\n# 1) IMPORTS\n# ============================================\nimport os\nfrom pathlib import Path\nimport numpy as np\nimport torch\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    MBartForConditionalGeneration,\n    MBart50TokenizerFast,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments\n)\nimport evaluate\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"  # disable WandB\n\n# ============================================\n# 2) LOAD DATASET\n# ============================================\ndataset_root = Path(\"/kaggle/input/parallel-corpus-for-english-urdu-language/Dataset\")\neng_file = dataset_root / \"english-corpus.txt\"\nurd_file = dataset_root / \"urdu-corpus.txt\"\n\n# Read lines\nwith open(eng_file, \"r\", encoding=\"utf-8\") as f:\n    eng_lines = [l.strip() for l in f.readlines()]\n\nwith open(urd_file, \"r\", encoding=\"utf-8\") as f:\n    urd_lines = [l.strip() for l in f.readlines()]\n\n# Ensure equal lengths\nn = min(len(eng_lines), len(urd_lines))\neng_lines = eng_lines[:n]\nurd_lines = urd_lines[:n]\n\ndataset = Dataset.from_dict({\"en\": eng_lines, \"ur\": urd_lines})\n\n# Train/validation/test split\nsplit = dataset.train_test_split(test_size=0.1, seed=42)\ninner = split['train'].train_test_split(test_size=0.1, seed=42)\ndata = DatasetDict({\n    \"train\": inner[\"train\"],\n    \"validation\": inner[\"test\"],\n    \"test\": split[\"test\"]\n})\n\nprint(\"Dataset sizes:\", {k: len(v) for k, v in data.items()})\n\n# ============================================\n# 3) LOAD mBART MODEL + TOKENIZER\n# ============================================\nMODEL = \"facebook/mbart-large-50-many-to-many-mmt\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntokenizer = MBart50TokenizerFast.from_pretrained(MODEL)\nmodel = MBartForConditionalGeneration.from_pretrained(MODEL).to(device)\n\nSRC_LANG = \"en_XX\"\nTGT_LANG = \"ur_PK\"\n\ntokenizer.src_lang = SRC_LANG\ntokenizer.tgt_lang = TGT_LANG\n\n# ============================================\n# 4) PREPROCESSING\n# ============================================\nMAX_SOURCE_LENGTH = 64  # reduce for speed\nMAX_TARGET_LENGTH = 64\n\ndef preprocess(batch):\n    model_inputs = tokenizer(\n        batch[\"en\"], padding=\"max_length\", truncation=True, max_length=MAX_SOURCE_LENGTH\n    )\n    labels = tokenizer(\n        batch[\"ur\"], padding=\"max_length\", truncation=True, max_length=MAX_TARGET_LENGTH\n    )\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized = data.map(preprocess, batched=True, remove_columns=data[\"train\"].column_names)\n\n# ============================================\n# 5) DATA COLLATOR\n# ============================================\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, label_pad_token_id=tokenizer.pad_token_id)\n\n# ============================================\n# 6) METRICS\n# ============================================\nbleu = evaluate.load(\"sacrebleu\")\n\ndef compute_metrics(eval_pred):\n    preds, labels = eval_pred\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    decoded_labels = [[l] for l in decoded_labels]\n    result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n    return {\"bleu\": result[\"score\"]}\n\n# ============================================\n# 7) TRAINING ARGUMENTS\n# ============================================\nOUT_DIR = \"/kaggle/working/mbart-en-ur\"\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=OUT_DIR,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    learning_rate=3e-5,\n    num_train_epochs=2,\n    logging_steps=100,\n    eval_steps=2000,\n    save_strategy=\"no\",\n    predict_with_generate=True,\n    fp16=True,\n    gradient_accumulation_steps=1,\n    report_to=\"none\"\n)\n\n\n# ============================================\n# 8) TRAINER\n# ============================================\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"validation\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n# ============================================\n# 9) TRAIN\n# ============================================\ntrainer.train()\ntrainer.save_model(OUT_DIR)\ntokenizer.save_pretrained(OUT_DIR)\n\n# ============================================\n# 10) EVALUATE\n# ============================================\ntest_result = trainer.evaluate(tokenized[\"test\"])\nprint(\"Test BLEU:\", test_result[\"eval_bleu\"])\n\n# ============================================\n# 11) TRANSLATION FUNCTION\n# ============================================\ndef translate(text_list):\n    tokenizer.src_lang = SRC_LANG\n    tokenizer.tgt_lang = TGT_LANG\n    enc = tokenizer(text_list, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SOURCE_LENGTH).to(device)\n    gen = model.generate(**enc, forced_bos_token_id=tokenizer.lang_code_to_id[TGT_LANG], max_length=MAX_TARGET_LENGTH, num_beams=5)\n    return tokenizer.batch_decode(gen, skip_special_tokens=True)\n\n# ============================================\n# 12) SAMPLE TRANSLATIONS\n# ============================================\nsample = data[\"test\"].select(range(10))\nen = [x[\"en\"] for x in sample]\nur_ref = [x[\"ur\"] for x in sample]\nur_pred = translate(en)\n\nfor i in range(len(en)):\n    print(f\"\\n--- Example {i+1} ---\")\n    print(\"English:\", en[i])\n    print(\"Urdu Reference:\", ur_ref[i])\n    print(\"Urdu Pred:\", ur_pred[i])\n\n\n# ============================================\n# 13) CUSTOM GENERATION TEST\n# ============================================\ncustom_text = [\"I am eating mangoes in the rain while coding Python.\"]\n\nprint(\"\\n================= CUSTOM TEST =================\")\nprint(\"English:\", custom_text[0])\n\ncustom_pred = translate(custom_text)\n\nprint(\"Urdu Prediction:\", custom_pred[0])\nprint(\"================================================\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# 0) INSTALL PACKAGES\n# ============================================\n!pip install transformers datasets evaluate sacrebleu sentencepiece --quiet\n\n# ============================================\n# 1) IMPORTS\n# ============================================\nimport os\nfrom pathlib import Path\nimport numpy as np\nimport torch\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    MBartForConditionalGeneration,\n    MBart50TokenizerFast,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments\n)\nimport evaluate\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"  # disable WandB\n\n# ============================================\n# 2) LOAD DATASET\n# ============================================\ndataset_root = Path(\"/kaggle/input/parallel-corpus-for-english-urdu-language/Dataset\")\neng_file = dataset_root / \"english-corpus.txt\"\nurd_file = dataset_root / \"urdu-corpus.txt\"\n\n# Read lines\nwith open(eng_file, \"r\", encoding=\"utf-8\") as f:\n    eng_lines = [l.strip() for l in f.readlines()]\n\nwith open(urd_file, \"r\", encoding=\"utf-8\") as f:\n    urd_lines = [l.strip() for l in f.readlines()]\n\n# Ensure equal lengths\nn = min(len(eng_lines), len(urd_lines))\neng_lines = eng_lines[:n]\nurd_lines = urd_lines[:n]\n\ndataset = Dataset.from_dict({\"en\": eng_lines, \"ur\": urd_lines})\n\n# Train/validation/test split\nsplit = dataset.train_test_split(test_size=0.1, seed=42)\ninner = split['train'].train_test_split(test_size=0.1, seed=42)\ndata = DatasetDict({\n    \"train\": inner[\"train\"],\n    \"validation\": inner[\"test\"],\n    \"test\": split[\"test\"]\n})\n\nprint(\"Dataset sizes:\", {k: len(v) for k, v in data.items()})\n\n# ============================================\n# 3) LOAD mBART MODEL + TOKENIZER\n# ============================================\nMODEL = \"facebook/mbart-large-50-many-to-many-mmt\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntokenizer = MBart50TokenizerFast.from_pretrained(MODEL)\nmodel = MBartForConditionalGeneration.from_pretrained(MODEL).to(device)\n\nSRC_LANG = \"en_XX\"\nTGT_LANG = \"ur_PK\"\n\ntokenizer.src_lang = SRC_LANG\ntokenizer.tgt_lang = TGT_LANG\n\n# ============================================\n# 4) PREPROCESSING\n# ============================================\nMAX_SOURCE_LENGTH = 64  # reduce for speed\nMAX_TARGET_LENGTH = 64\n\ndef preprocess(batch):\n    model_inputs = tokenizer(\n        batch[\"en\"], padding=\"max_length\", truncation=True, max_length=MAX_SOURCE_LENGTH\n    )\n    labels = tokenizer(\n        batch[\"ur\"], padding=\"max_length\", truncation=True, max_length=MAX_TARGET_LENGTH\n    )\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized = data.map(preprocess, batched=True, remove_columns=data[\"train\"].column_names)\n\n# ============================================\n# 5) DATA COLLATOR\n# ============================================\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, label_pad_token_id=tokenizer.pad_token_id)\n\n# ============================================\n# 6) METRICS\n# ============================================\nbleu = evaluate.load(\"sacrebleu\")\n\ndef compute_metrics(eval_pred):\n    preds, labels = eval_pred\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    decoded_labels = [[l] for l in decoded_labels]\n    result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n    return {\"bleu\": result[\"score\"]}\n\n# ============================================\n# 7) TRAINING ARGUMENTS\n# ============================================\nOUT_DIR = \"/kaggle/working/mbart-en-ur\"\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=OUT_DIR,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    learning_rate=3e-5,\n    num_train_epochs=2,\n    logging_steps=100,\n    eval_steps=2000,\n    save_strategy=\"no\",\n    predict_with_generate=True,\n    fp16=True,\n    gradient_accumulation_steps=1,\n    report_to=\"none\"\n)\n\n\n# ============================================\n# 8) TRAINER\n# ============================================\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"validation\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n# ============================================\n# 9) TRAIN\n# ============================================\ntrainer.train()\ntrainer.save_model(OUT_DIR)\ntokenizer.save_pretrained(OUT_DIR)\n\n# ============================================\n# 10) EVALUATE\n# ============================================\ntest_result = trainer.evaluate(tokenized[\"test\"])\nprint(\"Test BLEU:\", test_result[\"eval_bleu\"])\n\n# ============================================\n# 11) TRANSLATION FUNCTION\n# ============================================\ndef translate(text_list):\n    tokenizer.src_lang = SRC_LANG\n    tokenizer.tgt_lang = TGT_LANG\n    enc = tokenizer(text_list, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SOURCE_LENGTH).to(device)\n    gen = model.generate(**enc, forced_bos_token_id=tokenizer.lang_code_to_id[TGT_LANG], max_length=MAX_TARGET_LENGTH, num_beams=5)\n    return tokenizer.batch_decode(gen, skip_special_tokens=True)\n\n# ============================================\n# 12) SAMPLE TRANSLATIONS\n# ============================================\nsample = data[\"test\"].select(range(10))\nen = [x[\"en\"] for x in sample]\nur_ref = [x[\"ur\"] for x in sample]\nur_pred = translate(en)\n\nfor i in range(len(en)):\n    print(f\"\\n--- Example {i+1} ---\")\n    print(\"English:\", en[i])\n    print(\"Urdu Reference:\", ur_ref[i])\n    print(\"Urdu Pred:\", ur_pred[i])\n\n\n# ============================================\n# 13) CUSTOM GENERATION TEST\n# ============================================\ncustom_text = [\"I am eating mangoes in the rain while coding Python.\"]\n\nprint(\"\\n================= CUSTOM TEST =================\")\nprint(\"English:\", custom_text[0])\n\ncustom_pred = translate(custom_text)\n\nprint(\"Urdu Prediction:\", custom_pred[0])\nprint(\"================================================\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T03:33:45.564170Z","iopub.execute_input":"2025-11-25T03:33:45.564551Z","execution_failed":"2025-11-25T07:00:50.106Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-11-25 03:34:09.763643: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764041649.968157      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764041650.032396      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Dataset sizes: {'train': 19864, 'validation': 2208, 'test': 2453}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b078b977f0634d3ba6444989d7abf84c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b46f47e00fad4d2bb2607056d0810ba8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92c27d63a0d9427d8086730e8471a8f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30350e05c9d14d139f69eddf51c7d4e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"215a3a65bd7b4c12b14d0295af79eadc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4307b33d35834d56a34f1b55adef9b91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19864 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b93198211ad2482e95d3795052eca754"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2208 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c3890d41f0d4a4b938ccbe73ec98e9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2453 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"072ad026a9dd4d73b9878aabf3f5c32e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1fe8dea58da47a0be46d07d6e5c5ae6"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10309' max='19864' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10309/19864 2:20:52 < 2:10:35, 1.22 it/s, Epoch 1.04/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>3.399300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.208700</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.181300</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.190300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.182100</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.173500</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.130000</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.157300</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.162200</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.182900</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.153200</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.147100</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.143100</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.147900</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.143500</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.144900</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.168800</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.144400</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.129000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.140300</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.150900</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.131000</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.145800</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.137400</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.133100</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.132400</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.138800</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.124200</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.146600</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.147300</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.130600</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.126700</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.121300</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.132700</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.142000</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.128400</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.134400</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.138400</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.124700</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.135400</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.127700</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.125200</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.115900</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.108700</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.108300</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.124200</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.117500</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.120600</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.122400</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.106300</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.113500</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.100000</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>0.087000</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>0.117700</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.087800</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>0.119200</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>0.115800</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>0.114100</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>0.108300</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.110300</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>0.115100</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>0.115000</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>0.114000</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>0.107800</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.099600</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>0.106700</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>0.094500</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>0.096700</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>0.093300</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.102100</td>\n    </tr>\n    <tr>\n      <td>7100</td>\n      <td>0.121900</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>0.094200</td>\n    </tr>\n    <tr>\n      <td>7300</td>\n      <td>0.096900</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>0.112300</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.109200</td>\n    </tr>\n    <tr>\n      <td>7600</td>\n      <td>0.094300</td>\n    </tr>\n    <tr>\n      <td>7700</td>\n      <td>0.091800</td>\n    </tr>\n    <tr>\n      <td>7800</td>\n      <td>0.094100</td>\n    </tr>\n    <tr>\n      <td>7900</td>\n      <td>0.116000</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.100600</td>\n    </tr>\n    <tr>\n      <td>8100</td>\n      <td>0.108500</td>\n    </tr>\n    <tr>\n      <td>8200</td>\n      <td>0.094300</td>\n    </tr>\n    <tr>\n      <td>8300</td>\n      <td>0.099300</td>\n    </tr>\n    <tr>\n      <td>8400</td>\n      <td>0.086900</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.119700</td>\n    </tr>\n    <tr>\n      <td>8600</td>\n      <td>0.099700</td>\n    </tr>\n    <tr>\n      <td>8700</td>\n      <td>0.098300</td>\n    </tr>\n    <tr>\n      <td>8800</td>\n      <td>0.106800</td>\n    </tr>\n    <tr>\n      <td>8900</td>\n      <td>0.114100</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.096900</td>\n    </tr>\n    <tr>\n      <td>9100</td>\n      <td>0.089100</td>\n    </tr>\n    <tr>\n      <td>9200</td>\n      <td>0.085700</td>\n    </tr>\n    <tr>\n      <td>9300</td>\n      <td>0.103300</td>\n    </tr>\n    <tr>\n      <td>9400</td>\n      <td>0.077000</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.089000</td>\n    </tr>\n    <tr>\n      <td>9600</td>\n      <td>0.100600</td>\n    </tr>\n    <tr>\n      <td>9700</td>\n      <td>0.093000</td>\n    </tr>\n    <tr>\n      <td>9800</td>\n      <td>0.079900</td>\n    </tr>\n    <tr>\n      <td>9900</td>\n      <td>0.094000</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.069800</td>\n    </tr>\n    <tr>\n      <td>10100</td>\n      <td>0.046500</td>\n    </tr>\n    <tr>\n      <td>10200</td>\n      <td>0.060300</td>\n    </tr>\n    <tr>\n      <td>10300</td>\n      <td>0.063800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null}]}